{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Global Warming Exist? .ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOyHjr0Fu/sRVzopPJPytDf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adefirmanf/global-warming-sentiment/blob/main/Global_Warming_Exist%3F_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "060855w_vdFs",
        "outputId": "d3c576e1-3821-43f3-dbdc-935db15caf18",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "# Initialization \n",
        "import seaborn as sns\n",
        "import pandas as pd \n",
        "import numpy as np\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "\n",
        "from sklearn import model_selection, naive_bayes, svm\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer \n",
        "from sklearn.feature_extraction.text import CountVectorizer \n",
        "from sklearn import preprocessing\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from nltk.stem import WordNetLemmatizer \n",
        "from nltk.corpus import stopwords\n",
        "from nltk.sentiment import SentimentAnalyzer\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "nltk.download('stopwords')\n",
        "\n",
        "lemmatizer = WordNetLemmatizer() \n",
        "  "
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wsc9zy_OSL9A",
        "outputId": "d1879a9d-31d7-4f4b-dbdd-39457f398c29",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "# Fetching Data\n",
        "data = pd.read_csv('https://query.data.world/s/y6agr3tybufuvkmq7zvlmat27ehuz6', sep=',', encoding='windows-1252', engine='python')\n",
        "data_df = pd.DataFrame(data);\n",
        "    \n",
        "# Preprocessing Data (Removing invalid Y Data) - Phase 1 \n",
        "if 'existence.confidence' in data_df.columns:\n",
        "  data_df.pop('existence.confidence');\n",
        "\n",
        "\n",
        "# 1. Converting the {Y, Yes} = 1, {N, No} = 2\n",
        "yes_index = data_df[(data_df['existence'] == 'Y') | (data_df['existence'] == 'Yes') ].index\n",
        "no_index = data_df[(data_df['existence'] == 'N') | (data_df['existence'] == 'No') ].index\n",
        "\n",
        "data_df.loc[yes_index,'existence_int'] = 1\n",
        "data_df.loc[no_index,'existence_int'] = 0\n",
        "\n",
        "# 2. Remove NA & N/A Existence\n",
        "data_df = data_df.dropna()\n",
        "\n",
        "for i, v in data_df.iterrows():\n",
        "  # Lowering case\n",
        "  tweet = v['tweet'].lower()\n",
        "  \n",
        "  final_text = []\n",
        "  # Tokenize \n",
        "  stop_words = set(stopwords.words('english'))\n",
        "  tokenize = nltk.tokenize.word_tokenize(tweet)\n",
        "  for word in tokenize:\n",
        "    if word not in stopwords.words('english') and word.isalpha():\n",
        "      final_text.append(lemmatizer.lemmatize(word))\n",
        "  # Since panda doesn't recommending store the array data, we should join \n",
        "  # the data. \n",
        "  # Also we should remove the http/link keywords in last position\n",
        "  final_text.pop(len(final_text)-1)\n",
        "  data_df.loc[i, 'final_words'] = \" \".join(final_text)\n",
        "  \n",
        "X = data_df['final_words']\n",
        "Y = data_df['existence_int']\n",
        "\n",
        "Train_X, Test_X, Train_Y, Test_Y = model_selection.train_test_split(X, Y,test_size=0.3)\n",
        "\n",
        "Tfidf_vect = TfidfVectorizer(max_features=10000)\n",
        "c = Tfidf_vect.fit_transform(data_df['final_words'])\n",
        "\n",
        "Train_X_Tfidf = Tfidf_vect.transform(Train_X)\n",
        "Test_X_Tfidf = Tfidf_vect.transform(Test_X)\n",
        "\n",
        "print(Test_X)\n"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1840      book geoengineering change climate conversation\n",
            "2939    india seek clarity equity climate change flow ...\n",
            "2547    mrsjojoxx science disproves global warming ala...\n",
            "2172    make case climate change release glossy report...\n",
            "3694    climate change invasive specie http via climat...\n",
            "                              ...                        \n",
            "3374    rt josiedc due snow course rt senate global wa...\n",
            "5642    nasa climate change facebook become fan keep c...\n",
            "3537    rt davidcorndc someone please explain conserva...\n",
            "5826     evidence climate change cause earthquake volcano\n",
            "2116    plan b california brace climate change califor...\n",
            "Name: final_words, Length: 1268, dtype: object\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wJyKVCANdCP0",
        "outputId": "7a5f43bb-7d0b-41fd-939c-f2c93b81b3f2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# fit the training dataset on the NB classifier\n",
        "Naive = naive_bayes.MultinomialNB()\n",
        "Naive.fit(Train_X_Tfidf,Train_Y)\n",
        "# predict the labels on validation dataset\n",
        "predictions_NB = Naive.predict(Test_X_Tfidf)\n",
        "print(predictions_NB)\n",
        "# Use accuracy_score function to get the accuracy\n",
        "print(\"Naive Bayes Accuracy Score -> \",accuracy_score(predictions_NB, Test_Y)*100)"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1. 1. 1. ... 1. 1. 1.]\n",
            "Naive Bayes Accuracy Score ->  78.15457413249212\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SQo8Qz_Jg7cf",
        "outputId": "50eff92a-fe4a-4bfe-ead1-93426ba02fb6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "SVM = svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto')\n",
        "SVM.fit(Train_X_Tfidf, Train_Y)\n",
        "# predict the labels on validation dataset\n",
        "predictions_SVM = SVM.predict(Test_X_Tfidf)\n",
        "# Use accuracy_score function to get the accuracy\n",
        "print(\"SVM Accuracy Score -> \",accuracy_score(predictions_SVM, Test_Y)*100)"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SVM Accuracy Score ->  82.09779179810725\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kthbgmh1CTue",
        "outputId": "377f9b68-2437-4ab7-e737-0916366d99f7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "Manual_Test = pd.DataFrame(['rt'])\n",
        "Manual_Test_Tdidf = Tfidf_vect.transform(Manual_Test[0])\n",
        "\n",
        "SVM.predict(Manual_Test_Tdidf)"
      ],
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1.])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 101
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fc62sgOXKjrT",
        "outputId": "6728a0a8-f68b-43cd-e3dd-38a50c5981d7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "terms = Tfidf_vect.get_feature_names()\n",
        "\n",
        "# sum tfidf frequency of each term through documents\n",
        "sums = c.toarray().sum(axis=0)\n",
        "\n",
        "data = []\n",
        "for col, term in enumerate(terms):\n",
        "    data.append((term, sums[col]))\n",
        "\n",
        "ranking = pd.DataFrame(data, columns=['term','rank'])\n",
        "\n",
        "# Now the documents of the datasets mostly talked about\n",
        "# Global Warming.\n",
        "print(ranking.sort_values(by='rank', ascending=False))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "          term        rank\n",
            "2324    global  232.347570\n",
            "976    climate  227.263256\n",
            "5800   warming  218.307623\n",
            "871     change  216.914151\n",
            "2638      http  135.799525\n",
            "...        ...         ...\n",
            "2640     huang    0.270175\n",
            "6002   xiaoyan    0.270175\n",
            "3077       kun    0.270175\n",
            "2950  jingstri    0.260227\n",
            "3712     nolte    0.253574\n",
            "\n",
            "[6051 rows x 2 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}